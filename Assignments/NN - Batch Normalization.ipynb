{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06ba0ec",
   "metadata": {},
   "source": [
    "# Objective: The objective of this assignment is to assess students' understanding of batch normalization in artificial neural networks (ANN) and its impact on training performance.\n",
    "\n",
    "## Q1. Theory and Concepts:\n",
    "\n",
    "### 1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "Ans. Batch normalization is a technique used to stabilize and accelerate the training process of artificial neural networks. It involves normalizing the activations of each layer in a mini-batch before passing them to the next layer. By normalizing the inputs, batch normalization reduces internal covariate shift, which is the change in the distribution of network activations during training. This enables more stable and faster convergence during training.\n",
    "\n",
    "### 2. Describe the benefits of using batch normalization during training.\n",
    "Ans. The advantages of using batch normalization in neural networks include:\n",
    "a. Faster Convergence: Batch normalization reduces internal covariate shift, allowing the network to converge faster during training.\n",
    "b. Higher Learning Rates: It enables the use of higher learning rates, which can accelerate the optimization process.\n",
    "c. Reduced Dependency on Initialization: Batch normalization reduces the sensitivity of neural networks to weight initialization, making it easier to train deep networks.\n",
    "d. Regularization: Batch normalization acts as a form of regularization, reducing the need for dropout or other regularization techniques.\n",
    "e. Smoother Loss Landscape: It can lead to a smoother optimization landscape, making it less likely for the network to get stuck in poor local optima.\n",
    "\n",
    "### 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "Ans. Batch normalization involves two main steps during the forward pass:\n",
    "\n",
    "a. Normalization Step: For each mini-batch in training, the mean and variance of the activations in each layer are computed. The activations are then normalized using these statistics to have zero mean and unit variance.\n",
    "\n",
    "b. Learnable Parameters: Batch normalization introduces learnable parameters, gamma (scale) and beta (shift), for each normalized activation. These parameters allow the network to learn the optimal scale and shift for the normalized activations, which adds flexibility to the transformation.\n",
    "\n",
    "During the backward pass, the gradients for the learnable parameters (gamma and beta) and the input gradients are computed and used to update the parameters during the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eae229",
   "metadata": {},
   "source": [
    "## Q2. Implementation:\n",
    "\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., Tensorlow, PyTorch).\n",
    "3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "4. Implement batch normalization layers in the neural network and train the model again. \n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad543ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 170498071/170498071 [01:42<00:00, 1670870.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalize the CIFAR-10 dataset between -1 and 1 and apply data augmentation during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c6cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., Tensorlow, PyTorch).\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and define the loss function and optimizer\n",
    "net_no_bn = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_no_bn = optim.SGD(net_no_bn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa3952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and define the loss function and optimizer\n",
    "net_no_bn = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_no_bn = optim.SGD(net_no_bn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c030ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implement batch normalization layers in the neural network and train the model again. \n",
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Batch normalization after the first fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)  # Batch normalization after the second fully connected layer\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and define the loss function and optimizer\n",
    "net_with_bn = SimpleNNWithBN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_with_bn = optim.SGD(net_with_bn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7ef0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 10.69%\n",
      "Accuracy on test data: 11.45%\n"
     ]
    }
   ],
   "source": [
    "# 5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy on test data: {100 * correct / total}%\")\n",
    "\n",
    "test(net_no_bn)\n",
    "test(net_with_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace42c2",
   "metadata": {},
   "source": [
    "#6. Discuss the impact of batch normalization on the training process and the performance of the neural network.\n",
    "\n",
    "Batch normalization has a significant impact on the training process and the performance of neural networks. Let's discuss the effects of batch normalization in detail:\n",
    "\n",
    "Faster Convergence: Batch normalization helps in faster convergence during training. By normalizing the activations at each layer, it reduces the internal covariate shift, ensuring that the model can learn more efficiently. As a result, the number of epochs required to achieve a certain level of performance is generally reduced when using batch normalization.\n",
    "\n",
    "Stability during Training: Without batch normalization, the distribution of activations in deeper layers can shift during training, which can lead to the vanishing or exploding gradient problem. Batch normalization mitigates this problem by keeping the mean and variance of activations stable, allowing for smoother optimization and gradient flow.\n",
    "\n",
    "Higher Learning Rates: Batch normalization allows the use of higher learning rates during training. The normalization step helps in avoiding extreme weight updates, which can happen when using higher learning rates without batch normalization. Consequently, the optimization process becomes more stable and less prone to overshooting.\n",
    "\n",
    "Regularization Effect: Batch normalization acts as a form of regularization during training. The normalization process introduces noise into the network, which can help prevent overfitting. As a result, it reduces the reliance on dropout or other regularization techniques, simplifying the model architecture.\n",
    "\n",
    "Robustness to Initialization: Batch normalization reduces the sensitivity of neural networks to weight initialization. When training deep networks, initializing weights can be challenging. With batch normalization, the model is more robust to the initial weights, which can make it easier to train deep architectures effectively.\n",
    "\n",
    "Smoothing the Optimization Landscape: Batch normalization can lead to a smoother optimization landscape, making it less likely for the model to get stuck in poor local optima. The smoother landscape allows the optimizer to navigate through the loss surface more effectively, leading to better convergence.\n",
    "\n",
    "Despite the numerous advantages, it's essential to be aware of potential limitations or challenges associated with batch normalization:\n",
    "\n",
    "Batch Size Sensitivity: The effectiveness of batch normalization can be influenced by the batch size used during training. Smaller batch sizes can lead to less accurate estimation of batch statistics, reducing the effectiveness of batch normalization.\n",
    "\n",
    "Test-Time Behavior: During inference, batch normalization uses batch statistics for normalization. This can introduce some variance compared to training, especially when dealing with a single example at a time (e.g., during inference on a single test image). Techniques like running averages are often used during inference to mitigate this issue.\n",
    "\n",
    "Computational Overhead: Batch normalization adds some computational overhead due to the additional normalization and learnable parameters. However, the benefits it provides generally outweigh the computational cost.\n",
    "\n",
    "In conclusion, batch normalization is a powerful tool that significantly improves the training process and performance of neural networks. It helps in faster convergence, stabilizes training, and allows the use of higher learning rates, making it easier to train deep networks. By acting as a regularizer, it contributes to better generalization and reduces sensitivity to weight initialization. Despite its advantages, it's essential to consider batch size and test-time behavior while applying batch normalization in practice. Overall, batch normalization has become a standard technique used in modern deep learning architectures, contributing to the success of various state-of-the-art models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055af8c3",
   "metadata": {},
   "source": [
    "## Q3. Experimentation and Analysis\n",
    "### 1. Experiment with different batch sizes and observe the effect on the training dynamics and model performancer.\n",
    "Ans. Batch size is an important hyperparameter that affects the training dynamics and model performance. Try different batch sizes, such as 32, 64, and 128, and observe the following:\n",
    "\n",
    "Training speed: Larger batch sizes often result in faster training due to parallelization but may require more memory.\n",
    "Model performance: Smaller batch sizes may lead to more stochastic updates, potentially improving generalization, but can be noisier during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Experiment with Batch Size: 16\n",
      "Epoch 1, Loss: 1.8354947259902954\n",
      "Epoch 2, Loss: 1.6980251945495606\n",
      "Epoch 3, Loss: 1.6456050615501403\n",
      "Epoch 4, Loss: 1.6042463313865662\n",
      "Epoch 5, Loss: 1.5787264767837523\n",
      "\n",
      "Experiment with Batch Size: 32\n",
      "Epoch 1, Loss: 1.8000529370701472\n",
      "Epoch 2, Loss: 1.64314040562623\n",
      "Epoch 3, Loss: 1.584043623313489\n",
      "Epoch 4, Loss: 1.546454944674662\n",
      "Epoch 5, Loss: 1.5155206593808195\n",
      "\n",
      "Experiment with Batch Size: 64\n",
      "Epoch 1, Loss: 1.82094800716166\n",
      "Epoch 2, Loss: 1.6409826292406262\n",
      "Epoch 3, Loss: 1.574341952038543\n",
      "Epoch 4, Loss: 1.5349969856269525\n",
      "Epoch 5, Loss: 1.5028604619643267\n",
      "\n",
      "Experiment with Batch Size: 128\n",
      "Epoch 1, Loss: 1.872258763484028\n",
      "Epoch 2, Loss: 1.6787677494156392\n",
      "Epoch 3, Loss: 1.6063687990388602\n",
      "Epoch 4, Loss: 1.556219481446249\n",
      "Epoch 5, Loss: 1.5221031254819593\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalize the CIFAR-10 dataset between -1 and 1 and apply data augmentation during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# A list of batch sizes to experiment with\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nExperiment with Batch Size: {batch_size}\")\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    class SimpleNNWithBN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNNWithBN, self).__init__()\n",
    "            self.fc1 = nn.Linear(32*32*3, 512)\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, 256)\n",
    "            self.bn2 = nn.BatchNorm1d(256)\n",
    "            self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 32*32*3)\n",
    "            x = torch.relu(self.bn1(self.fc1(x)))\n",
    "            x = torch.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    # Initialize the model and define the loss function and optimizer\n",
    "    net_with_bn = SimpleNNWithBN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_with_bn = optim.SGD(net_with_bn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    def train_with_bn(net, criterion, optimizer, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    train_with_bn(net_with_bn, criterion, optimizer_with_bn, epochs=5)  # Training for 5 epochs for each batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367a36a",
   "metadata": {},
   "source": [
    "### 2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks.\n",
    "Ans. Advantages:\n",
    "a. Improved convergence: Batch normalization accelerates the training process, enabling the use of deeper networks.\n",
    "b. Generalization: It acts as a regularizer, reducing overfitting and improving the model's generalization to unseen data.\n",
    "c. Robustness to initialization: Batch normalization reduces the dependence on careful weight initialization, making training more straightforward.\n",
    "\n",
    "Potential Limitations:\n",
    "a. Computation overhead: Batch normalization adds some computational overhead due to the normalization step and additional learnable parameters.\n",
    "b. Batch size sensitivity: Smaller batch sizes can reduce the effectiveness of batch normalization, as accurate statistics are challenging to estimate.\n",
    "c. Test-time behavior: During inference, the normalization is based on the batch statistics, which can introduce some variance compared to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93f9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051462e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
