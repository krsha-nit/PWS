{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda53f86",
   "metadata": {},
   "source": [
    "### Q1. In order to predict house price based on several characteristics, such as location, square footage, number of bedrooms, etc., you are developing an SVM regression model. Which regression metric in this situation would be the best to employ?\n",
    "    Dataset link: https://drive.google.com/file/d/1Z9oLpmt6IDRNw7IeNcHYTGeJRYypRSC0/view?usp=share_link\n",
    "    \n",
    "Ans.  In order to predict house prices based on several characteristics, such as location, square footage, number of bedrooms, etc., the best regression metric to employ would be Mean Squared Error (MSE). MSE is a common metric for regression tasks and measures the average squared difference between the predicted house prices and the actual prices. It penalizes larger errors more, making it suitable for scenarios where accuracy is crucial, like predicting house prices.\n",
    "\n",
    "### Q2. You have built an SVM regression model and are trying to decide between using MSE or R-squared as your evaluation metric. Which metric would be more appropriate if your goal is to predict the actual price of a house as accurately as possible?\n",
    "Ans.  If the goal is to predict the actual price of a house as accurately as possible, Mean Squared Error (MSE) would be more appropriate as the evaluation metric. MSE directly measures the average squared difference between predicted and actual house prices, providing an indication of how well the model's predictions align with the true prices. Minimizing MSE is equivalent to finding the model that provides the most accurate predictions.\n",
    "\n",
    "### Q3. You have a dataset with a significant number of outliers and are trying to select an appropriate regression metric to use with your SVM model. Which metric would be the most appropriate in this scenario?\n",
    "Ans. If the dataset contains a significant number of outliers, the most appropriate regression metric to use with the SVM model would be Mean Absolute Error (MAE). MAE is less sensitive to outliers compared to MSE because it takes the absolute difference between predicted and actual values. In the presence of outliers, MSE can be heavily influenced by these extreme values, leading to a misleading evaluation of the model's performance. MAE provides a more robust measure of error in such scenarios.\n",
    "\n",
    "### Q4. You have built an SVM regression model using a polynomial kernel and are trying to select the best metric to evaluate its performance. You have calculated both MSE and RMSE and found that both values are very close. Which metric should you choose to use in this case?\n",
    "Ans.  If both Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) values are very close for the SVM regression model using a polynomial kernel, it is generally preferable to use RMSE as the evaluation metric. RMSE is the square root of MSE and provides a metric in the same units as the target variable (house prices in this case). RMSE offers a better understanding of the average prediction error and is more interpretable since it is on the same scale as the target variable.\n",
    "\n",
    "### Q5. You are comparing the performance of different SVM regression models using different kernels (linear, polynomial, and RBF) and are trying to select the best evaluation metric. Which metric would be most appropriate if your goal is to measure how well the model explains the variance in the target variable?\n",
    "Ans.  If the goal is to measure how well the SVM regression models explain the variance in the target variable, the most appropriate evaluation metric would be R-squared (R^2). R-squared represents the proportion of variance in the target variable that is explained by the model. A higher R-squared value indicates a better fit to the data and a better ability to explain the variability in the target variable. Therefore, R-squared is a suitable metric for comparing the performance of different SVM regression models with different kernels when the focus is on explaining the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9928430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 16867.40\n",
      "R-squared (R^2): 0.21\n",
      "Mean Absolute Error (MAE): 46.45\n",
      "Root Mean Squared Error (RMSE): 129.87\n",
      "R-squared (R^2) for Linear Kernel: 0.26\n",
      "R-squared (R^2) for Polynomial Kernel: 0.05\n",
      "R-squared (R^2) for RBF Kernel: 0.21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Bengaluru_House_Data.csv\")\n",
    "\n",
    "# Drop irrelevant columns (you can add or remove columns based on the analysis)\n",
    "data = data.drop([\"society\"], axis=1)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = ['bath', 'balcony']\n",
    "categorical_features = ['area_type', 'availability', 'location', 'size']\n",
    "\n",
    "# Handle missing values for numerical features with median imputation\n",
    "numerical_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Handle missing values for categorical features with constant imputation\n",
    "categorical_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"unknown\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Preprocess numerical features\n",
    "numerical_transformer = Pipeline(steps=[('imputer', numerical_imputer)])\n",
    "\n",
    "# Preprocess \"total_sqft\" column\n",
    "def preprocess_sqft(total_sqft):\n",
    "    if '-' in total_sqft:\n",
    "        sqft_values = total_sqft.split('-')\n",
    "        return (float(sqft_values[0]) + float(sqft_values[1])) / 2\n",
    "    try:\n",
    "        return float(total_sqft)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "data['total_sqft'] = data['total_sqft'].apply(preprocess_sqft)\n",
    "\n",
    "# Preprocess categorical features\n",
    "categorical_transformer = Pipeline(steps=[('imputer', categorical_imputer), ('encoder', encoder)])\n",
    "\n",
    "# Create column transformer to preprocess both numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(\"price\", axis=1)\n",
    "y = data[\"price\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the SVR model with preprocessor\n",
    "svr_model = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR())])\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions\n",
    "y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Q1: Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Q2: R-squared (R^2)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared (R^2): {r_squared:.2f}\")\n",
    "\n",
    "# Q3: Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# Q4: Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Q5: R-squared (R^2) for different kernels\n",
    "svr_linear = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(kernel='linear'))])\n",
    "svr_poly = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(kernel='poly'))])\n",
    "svr_rbf = Pipeline(steps=[('preprocessor', preprocessor), ('svr', SVR(kernel='rbf'))])\n",
    "svr_linear.fit(X_train, y_train)\n",
    "svr_poly.fit(X_train, y_train)\n",
    "svr_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_linear = svr_linear.predict(X_test)\n",
    "y_pred_poly = svr_poly.predict(X_test)\n",
    "y_pred_rbf = svr_rbf.predict(X_test)\n",
    "\n",
    "r_squared_linear = r2_score(y_test, y_pred_linear)\n",
    "r_squared_poly = r2_score(y_test, y_pred_poly)\n",
    "r_squared_rbf = r2_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"R-squared (R^2) for Linear Kernel: {r_squared_linear:.2f}\")\n",
    "print(f\"R-squared (R^2) for Polynomial Kernel: {r_squared_poly:.2f}\")\n",
    "print(f\"R-squared (R^2) for RBF Kernel: {r_squared_rbf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a45f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
