{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6efffde0",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "    Ans. Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features to a specific range, typically [0, 1]. It rescales the data by subtracting the minimum value and dividing by the range (maximum value minus minimum value) of the feature. The formula for Min-Max scaling is:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "    Ans. The Unit Vector technique, also known as vector normalization, is a feature scaling method that transforms data into a unit vector, i.e., it scales the feature values to have a magnitude of 1 while maintaining the direction of the original vector. It is commonly used in the context of machine learning algorithms that rely on distance metrics or vector operations.\n",
    "\n",
    "    Difference from Min-Max Scaling:\n",
    "    Min-Max scaling scales the features to a fixed range [0, 1], while the Unit Vector technique scales the feature vectors to have a magnitude of 1 but retains their original direction. The Unit Vector technique is suitable when the magnitude of the feature values is irrelevant, and the direction or relative relationships among features matter more.\n",
    "\n",
    "    Example:\n",
    "    Consider a dataset with two features: \"Height\" and \"Weight.\" After applying the Unit Vector technique, both \"Height\" and \"Weight\" will have a magnitude of 1 in the scaled feature space. The direction of each data point in the original space will be preserved in the scaled space.\n",
    "\n",
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "    Ans. Principle Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information or variance in the data. It achieves this by finding the principal components, which are orthogonal (uncorrelated) linear combinations of the original features.\n",
    "\n",
    "    The steps involved in PCA are as follows:\n",
    "\n",
    "    Standardization: Standardize the data by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "    Eigendecomposition: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "    Sort Eigenvalues: Sort the eigenvalues in descending order and choose the top k eigenvalues and their corresponding eigenvectors to form the principal components.\n",
    "\n",
    "    Projection: Project the original data onto the k-dimensional subspace spanned by the selected principal components.\n",
    "\n",
    "    PCA is commonly used in data preprocessing and feature extraction to reduce the dimensionality of the data while retaining the most critical information, which can be useful for visualization, noise reduction, or feeding the reduced data into machine learning models.\n",
    "\n",
    "    Example:\n",
    "    Suppose we have a dataset with three correlated features: \"Age,\" \"Income,\" and \"Education Level.\" By applying PCA, we can transform these three features into two principal components that capture the most significant variance in the data. These two principal components will be orthogonal to each other, and we can visualize the data in a lower-dimensional space while retaining most of the essential patterns.\n",
    "\n",
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "    Ans. Relationship between PCA and Feature Extraction, and Using PCA for Feature Extraction:\n",
    "    PCA can be used as a feature extraction technique to transform the original features into a new set of uncorrelated features called principal components. Unlike traditional feature selection techniques that select a subset of the original features, PCA creates entirely new features that are linear combinations of the original ones.\n",
    "\n",
    "    Feature Extraction using PCA:\n",
    "\n",
    "    Data Preprocessing: Standardize the data by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "    Eigendecomposition: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "    Sort Eigenvalues: Sort the eigenvalues in descending order and choose the top k eigenvalues and their corresponding eigenvectors to form the principal components.\n",
    "\n",
    "    Feature Transformation: Project the original data onto the k-dimensional subspace spanned by the selected principal components to obtain the new feature representation.\n",
    "\n",
    "    The new principal components are orthogonal to each other, meaning they are uncorrelated. They are ordered by the amount of variance they explain, with the first principal component explaining the most variance.\n",
    "\n",
    "    Example:\n",
    "    Suppose we have a dataset with five features: \"Height,\" \"Weight,\" \"Age,\" \"Income,\" and \"Education Level.\" Instead of using all five features, we can apply PCA to extract the top two principal components. These two components will be linear combinations of the original features, capturing the most significant variance in the data. We can use these two principal components as new features to represent the data in a lower-dimensional space.\n",
    "\n",
    "\n",
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The datasetcontains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling topreprocess the data.\n",
    "    Ans. Using Min-Max Scaling for Food Delivery Recommendation System:\n",
    "    In the context of building a recommendation system for a food delivery service, Min-Max scaling can be applied to preprocess the data, specifically when dealing with numerical features like \"price,\" \"rating,\" and \"delivery time.\"\n",
    "\n",
    "    The steps to use Min-Max scaling are as follows:\n",
    "\n",
    "    Identify Numerical Features: Identify the numerical features in the dataset that need to be scaled. In this case, it could be \"price,\" \"rating,\" and \"delivery time.\"\n",
    "\n",
    "    Data Preprocessing: Standardize the data by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "    Min-Max Scaling: Apply Min-Max scaling to each numerical feature separately. For each feature, subtract the minimum value and divide by the range (maximum value minus minimum value) to scale the values to the [0, 1] range.\n",
    "\n",
    "\n",
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "    Ans. To use PCA for dimensionality reduction in the stock price prediction project, follow these steps:\n",
    "\n",
    "    Data Preprocessing: Ensure that the dataset is cleaned and standardized. Standardization is important for PCA since it involves variance-based analysis.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix of the standardized dataset. The covariance matrix represents the relationships and covariances between different features.\n",
    "\n",
    "    Eigendecomposition: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "    Sort Eigenvalues: Sort the eigenvalues in descending order and choose the top k eigenvalues and their corresponding eigenvectors to form the principal components. The top k components should capture most of the variance in the data.\n",
    "\n",
    "    Feature Transformation: Project the original data onto the k-dimensional subspace spanned by the selected principal components to obtain the reduced feature representation.\n",
    "\n",
    "    By using PCA, you can reduce the dimensionality of the dataset while retaining most of the important information, which can lead to improved model training and prediction performance.\n",
    "\n",
    "\n",
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "    Ans. Min =1, Max = 20\n",
    "        1: 1-1/20-1 = 0\n",
    "        5: 5-1/20-1 = 0.21\n",
    "        10: 10-1/20-1 = 0.47\n",
    "        15: 15-1/20-1 = 0.74\n",
    "        20: 20-1/20-1 = 1\n",
    "        So, the Min-Max scaled dataset with values transformed to a range of -1 to 1 is:\n",
    "    [0,0.21,0.47,0.74,1]\n",
    "        \n",
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "    Ans. To perform feature extraction using PCA on the dataset [height, weight, age, gender, blood pressure], you need to follow the steps mentioned earlier for PCA:\n",
    "\n",
    "    Data Preprocessing: Standardize the dataset by subtracting the mean and scaling to unit variance. This ensures that all features are on the same scale.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix of the standardized dataset.\n",
    "\n",
    "    Eigendecomposition: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "    Sort Eigenvalues: Sort the eigenvalues in descending order and choose the top k eigenvalues and their corresponding eigenvectors to form the principal components.\n",
    "\n",
    "    The number of principal components you would choose to retain depends on the amount of variance you want to preserve in the data. You can determine the number of components based on the cumulative explained variance. For instance, if you want to retain 95% of the variance, you can choose the minimum number of principal components that contribute to this level of variance.\n",
    "\n",
    "    If the dataset contains five features, you will have five principal components. However, you can decide to retain fewer components based on the explained variance or by observing the scree plot (plot of eigenvalues) to see the drop-off in the explained variance as you move down the list of components.\n",
    "\n",
    "    For example, if the first three principal components explain 90% of the variance, you might choose to retain these three components, reducing the dataset to a three-dimensional subspace. This reduction in dimensionality can simplify the data representation and potentially improve the efficiency of subsequent machine learning models or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ccb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
