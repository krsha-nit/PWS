{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec972e77",
   "metadata": {},
   "source": [
    "#### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "    Ans. Missing values in a dataset refer to the absence of a particular value in a specific column or feature. This occurs when no data is recorded for that observation, and it is typically represented as \"NaN\" (Not a Number) or \"null\" in various programming languages.\n",
    "\n",
    "    Handling missing values is essential for several reasons:\n",
    "\n",
    "        Statistical accuracy: Missing values can introduce bias in statistical analyses and lead to incorrect conclusions.\n",
    "        Data quality: Missing values can impact the quality of machine learning models if not handled appropriately.\n",
    "        Model performance: Many machine learning algorithms cannot handle missing values directly and may throw errors or provide inaccurate results.\n",
    "        Predictive power: Missing data can lead to a reduction in the predictive power of a model.\n",
    "\n",
    "    Some algorithms that are not affected by missing values or can handle them directly are:\n",
    "\n",
    "        Decision Trees: Decision trees can naturally handle missing values during the split process.\n",
    "        Random Forest: Random Forest is an ensemble method based on decision trees and can handle missing values in a similar manner.\n",
    "        Gradient Boosting Machines: GBMs can also handle missing values like decision trees and random forests.\n",
    "        k-Nearest Neighbors (k-NN): k-NN can work with missing values by ignoring the missing feature when computing distances.\n",
    "        Naive Bayes: Naive Bayes algorithm can handle missing values by ignoring the missing attribute when calculating probabilities.\n",
    "\n",
    "\n",
    "#### Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "    Ans. There are several techniques to handle missing data in a dataset. Some common techniques are:\n",
    "\n",
    "    Removing Rows: Removing the rows with missing values is a straightforward approach, but it can lead to significant data loss \n",
    "    Mean/Median/Mode Imputation: Filling missing values with the mean, median, or mode of the non-missing values in the same column.\n",
    "    Forward Fill (or Backward Fill): Propagate the last observed non-missing value forward (or the next observed non-missing value backward) to fill in missing values.\n",
    "    Interpolation Methods: Using interpolation techniques like linear interpolation, polynomial interpolation, etc., to estimate missing values.\n",
    "    K-Nearest Neighbors Imputation: Using the values from the k-nearest neighbors to impute missing values.\n",
    "    Multiple Imputation: Generating multiple imputations for missing data and then combining them for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8f8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B      C\n",
      "0  1.0  7.5  10.00\n",
      "1  2.0  6.0  11.00\n",
      "2  3.0  7.0  12.00\n",
      "3  4.0  8.0  11.75\n",
      "4  5.0  9.0  14.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 6, 7, 8, 9],\n",
    "        'C': [10, 11, 12, None, 14]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation for missing values\n",
    "df_filled = df.fillna(df.mean())\n",
    "\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31f4fd",
   "metadata": {},
   "source": [
    "#### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "    Ans. Imbalanced data refers to a situation in a classification problem where the distribution of classes is not uniform. In other words, one class has a significantly larger number of samples compared to the other class(es). For example, in a binary classification problem, one class may have 90% of the samples, while the other class has only 10%.\n",
    "\n",
    "    The consequences of not handling imbalanced data can be severe:\n",
    "\n",
    "    Biased Model: The resulting model can be biased towards the majority class since it has more data to learn from.\n",
    "    Poor Generalization: The model may not generalize well to the minority class, leading to poor performance on new, unseen data.\n",
    "    Low Sensitivity: In scenarios where the minority class is of interest (e.g., detecting fraudulent transactions), the model's sensitivity to the minority class will be low.\n",
    "    Misleading Accuracy: Accuracy can be misleading in imbalanced datasets, as a model that predicts only the majority class may still have a high accuracy.\n",
    "    Loss of Information: Ignoring the minority class can lead to the loss of critical information.\n",
    "    Handling imbalanced data is crucial to build a robust and fair model that performs well on all classes.\n",
    "\n",
    "#### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "    Ans.Up-sampling and Down-sampling are two common techniques used to address imbalanced data:\n",
    "\n",
    "    Up-sampling: In up-sampling, the samples from the minority class are increased to match the number of samples in the majority class. This is typically done by duplicating existing samples or generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "    Down-sampling: In down-sampling, the samples from the majority class are reduced to match the number of samples in the minority class. This is done by randomly selecting a subset of samples from the majority class.\n",
    "\n",
    "    When to use each technique:\n",
    "    Up-sampling is useful when the minority class has a limited number of samples, and generating synthetic samples can help improve the model's ability to learn the minority class's patterns.\n",
    "    Down-sampling is suitable when the majority class has a significantly large number of samples, and removing some of the majority samples can help balance the class distribution.\n",
    "\n",
    "    Example:\n",
    "    Let's consider a binary classification problem with two classes, \"Normal\" (majority) and \"Anomaly\" (minority). The dataset has 90% normal samples and 10% anomaly samples, making it imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb40dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0         1        10\n",
      "1         2        20\n",
      "2         3        30\n",
      "3         4        40\n",
      "4         5        50\n",
      "5         3        30\n",
      "6         3        30\n",
      "7         3        30\n",
      "0     Normal\n",
      "1     Normal\n",
      "2    Anomaly\n",
      "3     Normal\n",
      "4     Normal\n",
      "5    Anomaly\n",
      "6    Anomaly\n",
      "7    Anomaly\n",
      "Name: Label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#up-sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with imbalanced data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [10, 20, 30, 40, 50],\n",
    "        'Label': ['Normal', 'Normal', 'Anomaly', 'Normal', 'Normal']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Apply RandomOverSampler to up-sample the minority class\n",
    "ros = RandomOverSampler(sampling_strategy='auto')\n",
    "X_upsampled, y_upsampled = ros.fit_resample(X, y)\n",
    "\n",
    "print(X_upsampled)\n",
    "print(y_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5da637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0         3        30\n",
      "1         4        40\n",
      "0    Anomaly\n",
      "1     Normal\n",
      "Name: Label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#down-sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with imbalanced data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'Feature2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "        'Label': ['Normal', 'Normal', 'Anomaly', 'Normal', 'Normal',\n",
    "                  'Normal', 'Normal', 'Normal', 'Normal', 'Normal']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Apply RandomUnderSampler to down-sample the majority class\n",
    "rus = RandomUnderSampler(sampling_strategy='auto')\n",
    "X_downsampled, y_downsampled = rus.fit_resample(X, y)\n",
    "\n",
    "print(X_downsampled)\n",
    "print(y_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279e4e9",
   "metadata": {},
   "source": [
    "#### Q5: What is data Augmentation? Explain SMOTE.\n",
    "    Ans. Data augmentation is a technique commonly used in machine learning to artificially increase the size of a dataset by generating additional data points from the existing data. This approach helps to overcome limitations caused by a small dataset and improves the model's generalization capabilities. Data augmentation involves applying various transformations to the original data, creating modified versions of the same sample while preserving its class label.\n",
    "\n",
    "    SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation method designed to address imbalanced datasets, where one class has significantly fewer samples than the other(s). SMOTE works by creating synthetic samples of the minority class to balance the class distribution. It generates synthetic examples for the minority class by interpolating between the feature vectors of two or more nearest neighbors belonging to the same class.\n",
    "\n",
    "    Here's how SMOTE works in a simplified example:\n",
    "\n",
    "    Let's say we have a binary classification problem with two classes, \"A\" (majority) and \"B\" (minority). The dataset contains three samples of class \"A\" and one sample of class \"B.\" In SMOTE, we select a sample from class \"B\" and find its k-nearest neighbors (e.g., k=3). Then, we generate two synthetic samples by randomly selecting two of these neighbors and taking a weighted average of their features.\n",
    "\n",
    "    The synthetic samples will be new data points similar to the existing minority class sample, but they will not be identical. This process is repeated until the desired balance between the two classes is achieved.\n",
    "\n",
    "    SMOTE helps to overcome the class imbalance problem and can lead to better model performance when the minority class is underrepresented.\n",
    "\n",
    "#### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "    Ans. Outliers are data points that significantly differ from the majority of the data in a dataset. They can be unusually high or low values, lying far away from the other data points, and may not follow the same patterns as the rest of the data. Outliers can occur due to various reasons, such as measurement errors, data entry mistakes, or rare events.\n",
    "\n",
    "    Handling outliers is crucial for several reasons:\n",
    "\n",
    "    Impact on Model Performance: Outliers can disproportionately influence statistical analysis and machine learning models, leading to biased results and suboptimal model performance.\n",
    "\n",
    "    Distortion of Data Distribution: Outliers can distort the data distribution and lead to inaccurate insights.\n",
    "\n",
    "    Impact on Normality Tests: Outliers can violate the assumptions of normality tests, affecting the validity of statistical analyses.\n",
    "\n",
    "    Robustness: Removing or mitigating outliers can improve the robustness of models, making them less sensitive to extreme values.\n",
    "\n",
    "    Handling outliers can involve various techniques, such as removing them, transforming the data, or using more robust statistical methods that are less influenced by outliers.\n",
    "\n",
    "#### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "    Ans.When dealing with missing data in customer data analysis, several techniques can be used to handle the gaps:\n",
    "\n",
    "    Removing Rows: If the missing data is limited and random, removing rows with missing values might be an option. However, this approach should be used with caution, as it can lead to significant data loss.\n",
    "\n",
    "    Mean/Median/Mode Imputation: Filling missing values with the mean, median, or mode of the non-missing values in the same column can be a simple approach.\n",
    "\n",
    "    Forward Fill (or Backward Fill): Propagating the last observed non-missing value forward (or the next observed non-missing value backward) can be used to fill in missing values when data has a temporal or sequential nature.\n",
    "\n",
    "    Interpolation Methods: Using interpolation techniques like linear interpolation or spline interpolation to estimate missing values based on surrounding data points.\n",
    "\n",
    "    K-Nearest Neighbors Imputation: Using the values from the k-nearest neighbors to impute missing values.\n",
    "\n",
    "    Multiple Imputation: Generating multiple imputations for missing data and then combining them for analysis.\n",
    "\n",
    "    The choice of which technique to use depends on the specific characteristics of the data, the extent of missingness, and the potential impact on the analysis.\n",
    "\n",
    "#### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "    Ans. When a small percentage of the data is missing, it's essential to assess whether the missingness is completely random or if there is a pattern to it. Some strategies to determine this are:\n",
    "\n",
    "    Visualizations: Create visualizations, such as bar plots or heatmaps, to check if there are patterns in the missing data across different features. Visual inspection can often reveal if missingness is related to specific variables or combinations of variables.\n",
    "\n",
    "    Missingness Summary: Calculate the percentage of missing values for each feature. Features with high percentages of missing values might indicate a specific pattern.\n",
    "\n",
    "    Statistical Tests: Conduct statistical tests to check if the missing data is dependent on other variables. For example, you can perform a chi-square test to assess the independence between missingness and other categorical variables.\n",
    "\n",
    "    Imputation Comparison: Compare the performance of different imputation methods. If missingness is not random, different imputation methods might lead to different results.\n",
    "\n",
    "    Domain Knowledge: Rely on domain knowledge to understand whether certain data might be missing systematically due to specific reasons or limitations.\n",
    "\n",
    "    By using a combination of these strategies, you can gain insights into the nature of missing data and decide on the most appropriate approach for handling it in your analysis.\n",
    "\n",
    "\n",
    "#### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "    Ans. When dealing with imbalanced datasets in a medical diagnosis project, evaluating the performance of your machine learning model requires special attention. Some strategies you can use include:\n",
    "\n",
    "    Confusion Matrix: Use a confusion matrix to get a detailed understanding of true positives, true negatives, false positives, and false negatives. This is particularly useful for evaluating the performance on both the majority and minority classes.\n",
    "\n",
    "    Precision and Recall: Precision (positive predictive value) and recall (sensitivity) are valuable metrics for imbalanced datasets. Focus on recall, as it represents the model's ability to correctly identify the minority class.\n",
    "\n",
    "    F1-Score: The F1-score balances precision and recall and is a suitable metric when both false positives and false negatives are equally important.\n",
    "\n",
    "    Area Under the ROC Curve (AUC-ROC): The AUC-ROC metric provides an aggregate evaluation of the model's performance across different probability thresholds. A high AUC-ROC indicates good separation between the classes.\n",
    "\n",
    "    Precision-Recall Curve: Plotting the precision-recall curve can help analyze the trade-off between precision and recall at different thresholds.\n",
    "\n",
    "    Stratified Cross-Validation: Use stratified cross-validation to ensure that each fold maintains the original class distribution, preventing biased evaluations.\n",
    "\n",
    "    Class Weighting: Assign higher weights to the minority class during model training to give it more importance.\n",
    "\n",
    "    Ensemble Methods: Explore ensemble methods like Random Forest or Gradient Boosting, as they can handle imbalanced datasets more effectively than single classifiers.\n",
    "\n",
    "    Remember that choosing the right evaluation metric depends on the specific context of the medical diagnosis and the consequences of false positives and false negatives.\n",
    "\n",
    "#### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "    Ans.To balance the dataset and down-sample the majority class when estimating customer satisfaction, you can use the following methods:\n",
    "\n",
    "    Random Under-sampling: Randomly select a subset of satisfied customers to match the size of the dissatisfied customers. This approach reduces the number of majority class samples.\n",
    "\n",
    "    Cluster Centroids: Use cluster centroids to down-sample the majority class by replacing groups of similar majority class samples with their centroids.\n",
    "\n",
    "    NearMiss Algorithm: NearMiss is an under-sampling technique that selects majority class samples based on their distance to minority class samples.\n",
    "\n",
    "    Tomek Links: Remove samples from the majority class that form Tomek links with the minority class. Tomek links are pairs of samples from different classes that are close to each other.\n",
    "\n",
    "    Edited Nearest Neighbors: Remove samples from the majority class that are misclassified by their k-nearest neighbors in the minority class.\n",
    "\n",
    "    These techniques help balance the dataset, allowing the model to give equal importance to both satisfied and dissatisfied customers during training.\n",
    "\n",
    "#### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "    Ans. To balance the dataset and up-sample the minority class when dealing with a rare event, you can use the following methods:\n",
    "\n",
    "    SMOTE (Synthetic Minority Over-sampling Technique): As mentioned earlier, SMOTE creates synthetic samples for the minority class by interpolating between the feature vectors of neighboring minority samples.\n",
    "\n",
    "    ADASYN (Adaptive Synthetic Sampling): ADASYN is an extension of SMOTE that generates synthetic samples with higher density in regions where the class distribution is sparse.\n",
    "\n",
    "    Random Over-sampling: Randomly duplicate samples from the minority class to increase their representation.\n",
    "\n",
    "    SMOTE with Tomek Links: Apply SMOTE and then use Tomek links to remove samples that form Tomek links between the minority and majority classes.\n",
    "\n",
    "    SMOTE with ENN (Edited Nearest Neighbors): Apply SMOTE and then use Edited Nearest Neighbors to remove noisy samples.\n",
    "\n",
    "    These techniques help balance the dataset by creating additional samples for the rare event, which improves the model's ability to recognize and predict the minority class. However, it's essential to be cautious with up-sampling, as it can potentially lead to overfitting, especially if the dataset is already small. Therefore, cross-validation and monitoring the model's performance on validation data are crucial when employing up-sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c2e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
