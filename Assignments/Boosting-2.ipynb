{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9eba91",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "Ans. Gradient Boosting Regression is a popular machine learning technique used for regression tasks. It is an extension of the gradient boosting algorithm, which is an ensemble learning method that combines multiple weak learners (usually decision trees) to create a stronger predictive model. In Gradient Boosting Regression, weak learners are trained sequentially, and each subsequent learner aims to correct the errors made by the previous ones.\n",
    "\n",
    "The basic idea behind Gradient Boosting Regression is to fit the weak learners to the negative gradient (or the \"residuals\") of the loss function with respect to the predictions of the previous model. This way, each new model focuses on learning the remaining patterns in the data that the previous models were unable to capture.\n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e588effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 208895804.20650816\n",
      "R-squared: -174079835.8387568\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the first model with the mean of the target values\n",
    "        self.models.append(np.mean(y))\n",
    "        residuals = y - self.models[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate negative gradients (residuals) for the current model\n",
    "            gradients = -residuals\n",
    "\n",
    "            # Train a decision tree on the negative gradients\n",
    "            tree = self.build_tree(X, gradients, depth=0)\n",
    "            self.models.append(tree)\n",
    "\n",
    "            # Update the residuals with the predictions from the new tree\n",
    "            residuals -= self.learning_rate * self.predict_tree(X, tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of weak learners\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models[1:]:\n",
    "            predictions += self.learning_rate * self.predict_tree(X, model)\n",
    "        return predictions + self.models[0]\n",
    "\n",
    "    def predict_tree(self, X, tree):\n",
    "        if isinstance(tree, tuple):\n",
    "            feature_index, threshold, left_tree, right_tree = tree\n",
    "            predictions = np.where(X[:, feature_index] < threshold, \n",
    "                                   self.predict_tree(X, left_tree),\n",
    "                                   self.predict_tree(X, right_tree))\n",
    "            return predictions\n",
    "        else:\n",
    "            return np.full(len(X), tree)\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(X) == 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_index, threshold, split_loss = self.find_best_split(X, y)\n",
    "\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth+1)\n",
    "\n",
    "        return (feature_index, threshold, left_tree, right_tree)\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        best_feature_index, best_threshold, best_split_loss = None, None, float('inf')\n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_loss = np.sum((y[left_mask] - np.mean(y[left_mask]))**2)\n",
    "                right_loss = np.sum((y[right_mask] - np.mean(y[right_mask]))**2)\n",
    "                split_loss = left_loss + right_loss\n",
    "\n",
    "                if split_loss < best_split_loss:\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_split_loss = split_loss\n",
    "\n",
    "        return best_feature_index, best_threshold, best_split_loss\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Sample data\n",
    "X_train = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([6, 7, 8]).reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_train, gb_regressor.predict(X_train)))\n",
    "print(\"R-squared:\", r_squared(y_train, gb_regressor.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318e068",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c1819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error (Test): 11.704449678631754\n",
      "R-squared (Test): -16.55667451794763\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the first model with the mean of the target values\n",
    "        self.models.append(np.mean(y))\n",
    "        residuals = y - self.models[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate negative gradients (residuals) for the current model\n",
    "            gradients = -residuals\n",
    "\n",
    "            # Train a decision tree on the negative gradients\n",
    "            tree = self.build_tree(X, gradients, depth=0)\n",
    "            self.models.append(tree)\n",
    "\n",
    "            # Update the residuals with the predictions from the new tree\n",
    "            residuals -= self.learning_rate * self.predict_tree(X, tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of weak learners\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models[1:]:\n",
    "            predictions += self.learning_rate * self.predict_tree(X, model)\n",
    "        return predictions + self.models[0]\n",
    "\n",
    "    def predict_tree(self, X, tree):\n",
    "        if isinstance(tree, tuple):\n",
    "            feature_index, threshold, left_tree, right_tree = tree\n",
    "            predictions = np.where(X[:, feature_index] < threshold, \n",
    "                                   self.predict_tree(X, left_tree),\n",
    "                                   self.predict_tree(X, right_tree))\n",
    "            return predictions\n",
    "        else:\n",
    "            return np.full(len(X), tree)\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(X) == 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        feature_index, threshold, split_loss = self.find_best_split(X, y)\n",
    "\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth+1)\n",
    "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth+1)\n",
    "\n",
    "        return (feature_index, threshold, left_tree, right_tree)\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        best_feature_index, best_threshold, best_split_loss = None, None, float('inf')\n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_loss = np.sum((y[left_mask] - np.mean(y[left_mask]))**2)\n",
    "                right_loss = np.sum((y[right_mask] - np.mean(y[right_mask]))**2)\n",
    "                split_loss = left_loss + right_loss\n",
    "\n",
    "                if split_loss < best_split_loss:\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_split_loss = split_loss\n",
    "\n",
    "        return best_feature_index, best_threshold, best_split_loss\n",
    "\n",
    "# Sample data\n",
    "X_train = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Define the Gradient Boosting Regressor with default hyperparameters\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define hyperparameters to search through\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([6, 7, 8]).reshape(-1, 1)\n",
    "y_test = np.array([6, 7, 8])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (Test):\", mse)\n",
    "print(\"R-squared (Test):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86ed14",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans. In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on the given training data. Typically, a weak learner is a decision tree with a small depth (often referred to as a \"decision stump\" if the depth is limited to only one split).\n",
    "\n",
    "The idea behind using weak learners in Gradient Boosting is that combining multiple weak learners can lead to a powerful ensemble model. Each weak learner focuses on learning the patterns that are not captured by the previous models, and their predictions are combined to form the final prediction of the ensemble.\n",
    "\n",
    "Using weak learners allows Gradient Boosting to iteratively learn and correct errors made by the ensemble so far, making the final model more accurate and robust.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans. The intuition behind the Gradient Boosting algorithm is to create a strong predictive model by combining multiple weak learners (usually decision trees) sequentially. The idea is to iteratively correct the errors made by the previous weak learners, ultimately leading to a powerful ensemble model. The key intuition can be summarized as follows:\n",
    "\n",
    "    Starting Point: The algorithm starts with a simple model that serves as the initial prediction for all instances in the training data. For regression tasks, this can be the mean of the target variable, while for classification tasks, it can be the majority class.\n",
    "    Error Correction: The algorithm then sequentially adds weak learners (e.g., decision trees) to the ensemble. Each weak learner is trained to focus on learning the patterns in the data that the previous models failed to capture. It does this by fitting to the negative gradients (residuals) of the loss function with respect to the current ensemble's predictions.\n",
    "    Weighted Combination: The predictions of all weak learners are combined in a weighted manner, where the weight assigned to each learner depends on its individual performance. More accurate learners are given higher weights in the ensemble.\n",
    "    Iterative Learning: The process continues for a predefined number of iterations or until a stopping criterion is met. Each subsequent weak learner emphasizes the mistakes made by the previous ensemble, leading to a more accurate and robust model.\n",
    "\n",
    "By combining multiple weak learners in an iterative manner, the Gradient Boosting algorithm can effectively learn complex relationships in the data and provide excellent predictive performance.\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans. The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. The steps involved in constructing the ensemble can be summarized as follows:\n",
    "\n",
    "    Start with a simple model: The process begins by initializing the ensemble with a simple model, which is usually the mean of the target variable (for regression) or the majority class (for classification). This initial model serves as the baseline prediction.\n",
    "    Calculate negative gradients: For each iteration, the algorithm calculates the negative gradients (also known as the \"residuals\") of the loss function with respect to the current ensemble's predictions. These gradients represent the errors made by the current ensemble on the training data.\n",
    "    Train a weak learner: A weak learner (e.g., decision tree) is trained on the negative gradients obtained in the previous step. The weak learner's goal is to learn and capture the patterns in the data that the current ensemble failed to model accurately.\n",
    "    Update the ensemble: The weak learner is added to the ensemble with a weight proportional to its performance (usually based on its accuracy). The weight determines the influence of the weak learner in the final prediction. Additionally, the predictions of the weak learner are combined with the predictions of the previous ensemble using the learning rate, which controls the step size of the iterative learning process.\n",
    "    Iterative learning: Steps 2 to 4 are repeated for a predefined number of iterations or until a stopping criterion is met. With each iteration, the weak learners focus on correcting the errors made by the current ensemble, leading to an ensemble that continually improves its predictive performance.\n",
    "\n",
    "By iteratively combining the predictions of weak learners and emphasizing the mistakes made by the ensemble so far, the Gradient Boosting algorithm constructs a powerful ensemble that can generalize well on the data and achieve high predictive accuracy.\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Ans. The mathematical intuition of the Gradient Boosting algorithm can be understood through the following steps:\n",
    "\n",
    "    Loss function: Define a differentiable loss function that measures the difference between the predicted values and the true target values. For example, for regression tasks, the commonly used loss function is the Mean Squared Error (MSE), and for binary classification tasks, it is the Log Loss (or Cross-Entropy Loss).\n",
    "    Initialize the ensemble: Initialize the ensemble with a simple model, which serves as the initial prediction for all instances in the training data. For regression tasks, this can be the mean of the target variable, while for binary classification tasks, it can be the log-odds (logit) of the positive class.\n",
    "    Negative gradients (residuals): Calculate the negative gradients (residuals) of the loss function with respect to the current ensemble's predictions. These gradients represent the errors made by the current ensemble on the training data.\n",
    "    Train a weak learner: Train a weak learner (e.g., decision tree) on the negative gradients obtained in the previous step. The weak learner's goal is to learn and capture the patterns in the data that the current ensemble failed to model accurately.\n",
    "    Weighted addition to the ensemble: Add the trained weak learner to the ensemble with a weight proportional to its performance (e.g., based on its accuracy). The weight determines the influence of the weak learner in the final prediction.\n",
    "    Update predictions: Update the predictions of the ensemble by adding the weighted predictions of the new weak learner. The learning rate controls the step size of the iterative learning process and balances the influence of each new weak learner.\n",
    "    Iterative learning: Repeat steps 3 to 6 for a predefined number of iterations or until a stopping criterion is met. During each iteration, the weak learners focus on correcting the errors made by the current ensemble, leading to an ensemble that continually improves its predictive performance.\n",
    "    Final prediction: The final prediction of the Gradient Boosting ensemble is the weighted sum of the predictions made by all the weak learners in the ensemble.\n",
    "\n",
    "By iteratively combining the predictions of weak learners and updating the ensemble based on the negative gradients of the loss function, the Gradient Boosting algorithm constructs a powerful ensemble model that can generalize well and achieve high predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f1438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
