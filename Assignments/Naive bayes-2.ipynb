{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d80fc61",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "Ans. To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let's define the events:\n",
    "\n",
    "    A: Employee uses the health insurance plan\n",
    "    B: Employee is a smoker\n",
    "\n",
    "We are given:\n",
    "\n",
    "    P(A) = 0.70 (Probability that an employee uses the health insurance plan)\n",
    "    P(B|A) = 0.40 (Probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "\n",
    "    P(B|A) (Probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "Using Bayes' theorem:\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "We don't have the direct probabilities P(A|B) or P(B), but we can calculate P(A) using the law of total probability:\n",
    "\n",
    "P(A) = P(A|B') * P(B') + P(A|B) * P(B)\n",
    "where B' represents the event of an employee not being a smoker.\n",
    "\n",
    "Since we know that P(B) = 0.70 (complement of P(B')) and P(A|B) = 0.40, we can calculate P(A):\n",
    "\n",
    "    P(A) = P(A|B') * (1 - P(B)) + P(A|B) * P(B)\n",
    "    P(A) = 0 * (1 - 0.70) + 0.40 * 0.70\n",
    "    P(A) = 0.28\n",
    "\n",
    "Now, we can find P(B|A) using Bayes' theorem:\n",
    "\n",
    "    P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "    P(B|A) = (0.40 * 0.70) / 0.28\n",
    "    P(B|A) = 0.5714 (approximately)\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%.\n",
    "\n",
    "\n",
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Ans. The difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle:\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is used for binary feature data, where each feature can take only one of two possible values, typically 0 or 1. It assumes that each feature is conditionally independent of others given the class label. It is commonly used in text classification tasks, where each term either occurs (1) or does not occur (0) in a document.\n",
    "\n",
    "Multinomial Naive Bayes: This classifier is used for discrete feature data, such as word counts or frequency data in text classification tasks. It can handle multiple discrete feature values and calculates the probability of observing a particular value for each class. It also assumes conditional independence between features given the class label.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary feature data, while Multinomial Naive Bayes is appropriate for discrete feature data with multiple values.\n",
    "\n",
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Ans.  Bernoulli Naive Bayes can handle missing values in a straightforward way. When a feature is missing for an instance in the training or testing data, it is treated as if the feature is not present (0) for that particular instance. This assumption aligns with the nature of Bernoulli Naive Bayes, where the features are binary (1 or 0).\n",
    "\n",
    "By considering the missing features as non-existent (0), the classifier can still calculate the probabilities of each feature being present or absent for each class, and use Bayes' theorem to make predictions based on the available features.\n",
    "\n",
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "Ans. Yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, the features are assumed to follow a Gaussian (normal) distribution for each class. It works well with continuous feature data.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes extends the model to accommodate multiple classes. When given a new instance with continuous feature values, the classifier calculates the probability of the instance belonging to each class using the Gaussian probability density function for each feature. It then selects the class with the highest probability as the predicted class for that instance.\n",
    "\n",
    "In summary, Gaussian Naive Bayes is applicable for both binary and multi-class classification problems, as long as the features are assumed to be continuous and follow a Gaussian distribution for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3000fd0",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "    \n",
    "    Accuracy\n",
    "    Precision\n",
    "    Recall\n",
    "    F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de3a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bernoulli Naive Bayes:\n",
      "Accuracy: 0.89\n",
      "Precision: 0.88\n",
      "Recall: 0.81\n",
      "F1 Score: 0.85\n",
      "-------------------------------------------------\n",
      "Evaluating Multinomial Naive Bayes:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.74\n",
      "Recall: 0.71\n",
      "F1 Score: 0.72\n",
      "-------------------------------------------------\n",
      "Evaluating Gaussian Naive Bayes:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.70\n",
      "Recall: 0.96\n",
      "F1 Score: 0.81\n",
      "-------------------------------------------------\n",
      "Accuracy on Test Set: 0.79\n",
      "Precision on Test Set: 0.76\n",
      "Recall on Test Set: 0.72\n",
      "F1 Score on Test Set: 0.74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [\n",
    "    f'feature_{i}' for i in range(57)\n",
    "] + ['spam']  # Assuming the last column is the target variable (spam or non-spam)\n",
    "data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('spam', axis=1)\n",
    "y = data['spam']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate each classifier's performance\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "classifier_names = ['Bernoulli Naive Bayes', 'Multinomial Naive Bayes', 'Gaussian Naive Bayes']\n",
    "\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    print(f\"Evaluating {classifier_names[i]}:\")\n",
    "    accuracy_scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: {:.2f}\".format(np.mean(accuracy_scores)))\n",
    "\n",
    "    precision_scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='precision')\n",
    "    print(\"Precision: {:.2f}\".format(np.mean(precision_scores)))\n",
    "\n",
    "    recall_scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='recall')\n",
    "    print(\"Recall: {:.2f}\".format(np.mean(recall_scores)))\n",
    "\n",
    "    f1_scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='f1')\n",
    "    print(\"F1 Score: {:.2f}\".format(np.mean(f1_scores)))\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "# Training the best performing model on the entire training set and evaluating on the test set\n",
    "best_model = multinomial_nb  # Replace with the best performing model\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Accuracy on Test Set: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Precision on Test Set: {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Recall on Test Set: {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score on Test Set: {f1_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e602f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
